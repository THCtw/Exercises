{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PPO_LunarLander.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMzMPl8a9ruW/by+Nq4q8cb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WGtE_YwaB5-E","colab_type":"text"},"source":["# **Proximal Policy Optimization**\n","\n","Environment: Lunar Lander"]},{"cell_type":"markdown","metadata":{"id":"wPLqDX2ypB0K","colab_type":"text"},"source":["## References\n","\n","#### Papers\n","- [Proximal Policy Optimization Algorithms, Schulman et al. 2017](https://arxiv.org/abs/1707.06347)\n","- [Emergence of Locomotion Behaviours in Rich Environments, Heess et al. 2017](https://arxiv.org/abs/1707.02286)\n","\n","#### Blogs\n","- [OpenAI Spinning Up - Proximal Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/ppo.html)\n","\n","#### Others\n","- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/index.html)\n","- [OpenAI Gym](https://gym.openai.com/)"]},{"cell_type":"markdown","metadata":{"id":"hCTdFexBqAjO","colab_type":"text"},"source":["## Preparation"]},{"cell_type":"code","metadata":{"id":"oKgu1rhNqEHn","colab_type":"code","colab":{}},"source":["%%capture\n","!sudo apt update\n","!sudo apt install python-opengl xvfb -y\n","!pip install gym[box2d] pyvirtualdisplay piglet tqdm\n","\n","%%capture\n","from pyvirtualdisplay import Display\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","from IPython import display\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","from tqdm import tqdm_notebook"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmEcTtE8qG2t","colab_type":"code","colab":{}},"source":["# Import gym and create a Lunar Lander environment\n","# Observation/State: Box(8,)\n","# Action: Discrete(4)\n","%%capture\n","import gym\n","env = gym.make('LunarLander-v2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bigUrXAHqH8T","colab_type":"text"},"source":["## PPO Algorithm\n","Proximal Policy Optimization algorithm from the [original paper](https://arxiv.org/abs/1707.06347)"]},{"cell_type":"markdown","metadata":{"id":"Bio4tjf0qWSz","colab_type":"text"},"source":["### Pseudocode\n","\n","1. Input: initial policy parameters $\\theta_0$, initial value function parameters $\\phi_0$\n","\n","2. **For** $k = 0, 1, 2, ...$ **do**\n","\n","3. > Collect set of trajectories $D_k = \\{\\tau_i \\}$ by running policy $\\pi_k = \\pi(\\theta_k)$ in the environment.\n","\n","4. > Compute rewards-to-go $\\hat{R}_t$.\n","\n","5. > Compute advantage estimates, $\\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\\phi_k}$.\n","\n","6. > Update the policy by maximizing the PPO-Clip objective:\n","<br>\n","$$\\theta_{k+1} = arg\\ \\max_{\\theta}\\ \\frac{1}{|D_k| T} \\sum_{\\tau \\in D_k} \\sum_{t=0}^T \\min \\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_k}(a_t|s_t)} A^{\\pi_{\\theta_k}}(s_t, a_t),\\ g(\\epsilon, A^{\\pi_{\\theta_k}}(s_t, a_t)) \\right),$$\n","<br>\n","typically via stochastic gradient ascent with Adam.\n","\n","7. > Fit value function by regression on mean-squared error:\n","<br>\n","$$\\phi_{k+1} = arg\\ \\min_{\\phi} \\frac{1}{|D_k| T} \\sum_{\\tau \\in D_k} \\sum_{t=0}^T {\\left( V_\\phi(s_t) - \\hat{R}_t \\right)}^2,$$\n","<br>\n","typically via some gradient descent algorithm.\n","\n","8. **end for**"]},{"cell_type":"markdown","metadata":{"id":"cxzQUix5qZ5M","colab_type":"text"},"source":["###  Policy Gradient Network\n","\n","> In this notebook, we implement two policy gradient networks, the one used as an example in hw15 of HY-Lee's course (PGNetLee) and the one in the [original paper](https://arxiv.org/abs/1707.06347) (PGNet)."]},{"cell_type":"code","metadata":{"id":"JjbNXHGkqTag","colab_type":"code","colab":{}},"source":["class PGNetLee(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(8, 16)\n","        self.fc2 = nn.Linear(16, 16)\n","        self.fc3 = nn.Linear(16, 4)\n","\n","    def forward(self, state):\n","        hid = torch.tanh(self.fc1(state))\n","        hid = torch.tanh(self.fc2(hid))\n","        return F.softmax(self.fc3(hid), dim=-1)\n","\n","\n","#class PGNet(nn.Module):\n","\n","#    def __init__(self):\n","#       super().__init__()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d5DR6UY4qnwM","colab_type":"text"},"source":["### Proximal Policy Optimization Agent\n","\n","In original paper, the authors compare several different surrogate objectives, including clipping $L^{CLIP}$,$\\ $ conservitive policy iteration (no clippling or penalty) $L^{CPI}$ and KL penalty (fixed and adaptive) $L^{PENfix}$ $L^{PENadp}$.$\\ $ Here we implement these different objectives in different agents."]},{"cell_type":"markdown","metadata":{"id":"m2jiW-x4wGeA","colab_type":"text"},"source":["#### Conservitive Policy Iteration"]},{"cell_type":"code","metadata":{"id":"F0N-qCb2yd43","colab_type":"code","colab":{}},"source":["class PPOAgentCPI():\n","\n","    def __init__(self, network):\n","        self.network = network"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acFWboRVwLiP","colab_type":"text"},"source":["#### Clipping"]},{"cell_type":"code","metadata":{"id":"wvaqjO7wwbsF","colab_type":"code","colab":{}},"source":["class PPOAgentCLIP():\n","\n","    def __init__(self, network):\n","        self.network = network"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f2hPL2XkwPsW","colab_type":"text"},"source":["#### KL Penalty (Fixed)\n"]},{"cell_type":"code","metadata":{"id":"p5cW6mL3ygac","colab_type":"code","colab":{}},"source":["class PPOAgentKLFix():\n","\n","    def __init__(self, network):\n","        self.network = network"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nc5GyxeLwVZd","colab_type":"text"},"source":["#### KL Penalty (Adpative)"]},{"cell_type":"code","metadata":{"id":"xcmJHEmIyjRk","colab_type":"code","colab":{}},"source":["class PPOAgentKLAdp():\n","\n","    def __init__(self, network):\n","        self.network = network"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"prqx0dF0zp0-","colab_type":"text"},"source":["### Estimator"]},{"cell_type":"markdown","metadata":{"id":"GQyty9s5COuo","colab_type":"text"},"source":["#### Value Function Network"]},{"cell_type":"code","metadata":{"id":"TCm2P7rDzzdi","colab_type":"code","colab":{}},"source":["#class valueNet(nn.Module):"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"reYKdsgTCm72","colab_type":"text"},"source":["#### Generalized Advantage Estimation Agent"]},{"cell_type":"code","metadata":{"id":"yf9KDKoaCrf-","colab_type":"code","colab":{}},"source":["#class GAEAgent():"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1lXjiJamqxej","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"CopqOkC80Fn0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lycqaM-jqzBU","colab_type":"text"},"source":["### Test"]},{"cell_type":"code","metadata":{"id":"SXIMCdDdqz4H","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}