{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TRPO_LunarLander.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOlS5UC+8bWEMIkOT3hvljy"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nhpEWJtKf6xG","colab_type":"text"},"source":["# **Trust Region Policy Optimization**\n","\n","Environment: Lunar Lander"]},{"cell_type":"markdown","metadata":{"id":"RwQi6GqRgcB3","colab_type":"text"},"source":["## References\n","\n","#### Papers\n","- [Trust Region Policy Optimization, Schulman et al. 2015](https://arxiv.org/abs/1502.05477)\n","- [High-Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al, 2015. Algorithm: GAE.](https://arxiv.org/abs/1506.02438)\n","- [Approximately Optimal Approximate Reinforcement Learning, Kakade and Langford 2002](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf)\n","\n","#### Blogs\n","- [OpenAI Spinning Up - Trust Region Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/trpo.html)\n","\n","#### Others\n","- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/index.html)\n","- [OpenAI Gym](https://gym.openai.com/)"]},{"cell_type":"markdown","metadata":{"id":"Mtvl_qfdhEGD","colab_type":"text"},"source":["## Preparation"]},{"cell_type":"code","metadata":{"id":"7JPQqFATf9ix","colab_type":"code","colab":{}},"source":["%%capture\n","!sudo apt update\n","!sudo apt install python-opengl xvfb -y\n","!pip install gym[box2d] pyvirtualdisplay piglet tqdm\n","\n","%%capture\n","from pyvirtualdisplay import Display\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","from IPython import display\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","from tqdm import tqdm_notebook"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SkF5P95Yg9CV","colab_type":"code","colab":{}},"source":["# Import gym and create a Lunar Lander environment\n","# Observation/State: Box(8,)\n","# Action: Discrete(4)\n","%%capture\n","import gym\n","env = gym.make('LunarLander-v2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czBoaD93lUQ6","colab_type":"text"},"source":["## TRPO Algorithm\n","Trust Region Policy Optimization algorithm from the [original paper](https://arxiv.org/abs/1502.05477)\n","\n","> Initialize $\\pi_0$.\n","> <br>**for** i = 0, 1, 2,... until convergence **do**\n","\n","> > Compute all advantage values $A_{\\pi_i}(s, a)$. \n","> > <br> Solve the constrained optimization problem:\n","> > <br>\n","> > <br> $\\pi_{i+1} = \\underset{\\pi}{arg\\ max}\\ L_{\\pi_i}(\\pi)$ $~~~~$ s.t. $\\bar{D}^{\\rho_{\\pi_i}}_{KL}(\\pi_i, \\pi) \\leq \\delta$\n","> > <br> where $L_{\\pi_i}(\\pi) = \\eta(\\pi_i) + \\underset{s}{\\sum} \\rho_{\\pi_i}(s) \\underset{a}{\\sum} \\pi(a|s) A_{\\pi_i}(s, a)$\n","\n","> > which is equivalent to solve:\n","> > <br> $\\underset{\\theta}{maximize}\\ \\underset{s}{\\sum} \\rho_{\\theta_{old}}(s) \\underset{a}{\\sum} \\pi_\\theta(a|s) A_{\\theta_{old}}(s, a)$ $~~~~$ s.t. $\\bar{D}^{\\rho_{\\theta_{old}}}_{KL}(\\theta_{old}, \\theta) \\leq \\delta$\n","\n","> **end for**\n","\n","<br> Next, we are trying to approximate the objective and constraint functions using Monte Carlo simulation.\n","\n","1. Replace $\\underset{s}{\\sum} \\rho_{\\theta_{old}}(s)[...]$ by $\\frac{1}{1-\\gamma}\\mathbb{E}_{s \\sim \\rho_{\\theta_{old}}}[...]$.\n","2. Replace advantage values function $A_{\\theta_{old}}$ by the state-action value function $Q_{\\theta_{old}}$.\n","3. Replace sum over actions by importance sampling estimator ($q$ is the sampling distribution):\n","> $\\underset{a}{\\sum} \\pi_\\theta(a|s) A_{\\theta_{old}}(s_n, a) = \\mathbb{E}_{a \\sim q}\\ [\\frac{\\pi_{\\theta}(a|s_n)}{q(a|s_n)} A_{\\theta_{old}}(s_n, a)]$\n","\n","<br> The optimization problem become:\n","\n","> $\\underset{\\theta}{maximize}\\ \\mathbb{E}_{s \\sim \\rho_{\\theta_{old}},\\ a \\sim q}\\ [\\frac{\\pi_{\\theta}(a|s)}{q(a|s)} Q_{\\theta_{old}}(s, a)]$ $~~~~$ s.t. $\\mathbb{E}_{s \\sim \\rho_{\\theta_{old}}}\\ [\\ D_{KL}(\\ \\pi_{\\theta_{old}}(\\cdot|s)\\ \\|\\ \\pi_{\\theta}(\\cdot|s)\\ )\\ ] \\leq \\delta$\n","\n","<br> The remaining steps are:\n","\n","1. Replace the expectations by sample averages.\n","2. Replace the $Q$ value by an empirical estimate.\n","\n","In the original paper, the author provided two schemes, single path and vine.\n","\n","* `Single Path`\n","<br> In this scheme, we generate a trajectory ($s_0, a_0, ... , s_{T-1}, a_{T-1}, s_T$) by $\\rho_0$ and $\\pi_{old}$, which means $p(a|s) = \\pi_{old}(a|s)$, and compute the $Q$ value as $$\\hat{\\mathcal{Q}}_{\\theta_{old}}(s_t, a_t) = \\sum_{\\tau\\in\\theta_{old}} \\sum_{l=0}^T \\gamma^lr(s_{t+l})$$\n","\n","* `Vine`\n","<br> In this scheem, "]},{"cell_type":"markdown","metadata":{"id":"-Z1fZMe1bkAF","colab_type":"text"},"source":["### Approximations\n","The theoretical TRPO,\n","\n","<br> $\\underset{\\theta}{maximize}\\ \\mathbb{E}_{s \\sim \\rho_{\\theta_{old}},\\ a \\sim q}\\ [\\frac{\\pi_{\\theta}(a|s)}{q(a|s)} Q_{\\theta_{old}}(s, a)]$ $~~~~$ s.t. $\\mathbb{E}_{s \\sim \\rho_{\\theta_{old}}}\\ [\\ D_{KL}(\\ \\pi_{\\theta_{old}}(\\cdot|s)\\ \\|\\ \\pi_{\\theta}(\\cdot|s)\\ )\\ ] \\leq \\delta$,\n","\n","<br> is still not easy to work with. Therefore, in Appendix C of the [original paper](https://arxiv.org/abs/1502.05477), the authors make additional approximations to solve the optimization problem efficiently.\n","\n","1. `Compute a search direction`\n","> Compute a search direction, using a linear approximation to objective and quagratic approximation to the constraint.\n","\n","2. `Line search`\n","> Perform a line search in that direction, ensuring that we improve the nonlinear objective while satisfying the nonlinear constraint."]},{"cell_type":"markdown","metadata":{"id":"YPF4Hy75mIJJ","colab_type":"text"},"source":["### Pseudocode\n","In the [original paper](https://arxiv.org/abs/1502.05477), the advantage estimates $\\hat{A}_t$ is $Q$ value $\\hat{\\mathcal{Q}}_{\\theta_{old}}(s_t, a_t)$.\n","<br> <br>\n","\n","1. Input: initail policy parameters $\\theta_0$, initial value function parameters $\\phi_0$.\n","\n","2. Hyperparameters: KL-divergence limit $\\delta$, backtracking coefficient $\\alpha$, maximum number of backtracking steps $K$.\n","\n","3. **for** $k=0, 1, 2, ...$ **do**\n","\n","4. > Collect set of trajectories $D_k = {\\{\\tau_i\\}}$ by running policy $\\pi_k = \\pi(\\theta_k)$ in the environment. (Single Path or Vine) \n","\n","5. > Compute rewards-to-go $\\hat{R}_t$.\n","\n","6. > Compute advantage estimates, $\\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\\phi_k}$.\n","\n","7. > Estimate policy gradient as \n","\n","<br> \n","    $$\\hat{g}_k = \\frac{1}{|D_k|} \\sum_{\\tau \\in D_k} \\sum_{t=0}^T \\ \\nabla_\\theta\\ log\\ \\pi_\\theta(a_t|s_t)|_{\\theta_k}\\ \\hat{A}_t.$$\n","<br>\n","\n","8. > Use the conjugate gradient algorithm to compute \n","\n","<br> \n","    $$\\hat{x}_k$$ \n","<br>"]},{"cell_type":"markdown","metadata":{"id":"9F-S6BEEDZPY","colab_type":"text"},"source":["### Policy Gradient Network\n","\n","> In this notebook, we implement two policy gradient networks, the one used as an example in hw15 of HY-Lee's course (PGNetLee) and the one in the [original paper](https://arxiv.org/abs/1502.05477) (PGNet)."]},{"cell_type":"code","metadata":{"id":"Fv7wff7vlXPI","colab_type":"code","colab":{}},"source":["class PGNetLee(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(8, 16)\n","        self.fc2 = nn.Linear(16, 16)\n","        self.fc3 = nn.Linear(16, 4)\n","\n","    def forward(self, state):\n","        hid = torch.tanh(self.fc1(state))\n","        hid = torch.tanh(self.fc2(hid))\n","        return F.softmax(self.fc3(hid), dim=-1)\n","\n","\n","#class PGNet(nn.Module):\n","\n","#    def __init__(self):\n","#       super().__init__()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-itJ6AREMEq","colab_type":"text"},"source":["### Trust Region Policy Optimization Agent\n","\n","> Next, we build an TRPO agent which take the policy gradient network abouve to take action and have the following functions:\n","1. `learn()`: Update the policy network with log probabilities and rewards.\n","2. `sample()`: Sample an action and return the log probabilities by the policy network with the observation from the environment."]},{"cell_type":"code","metadata":{"id":"hnLlIwRwD_1z","colab_type":"code","colab":{}},"source":["class TRPOAgent():\n","\n","    def __init__(self, network, gamma=0.9):\n","        self.network = network\n","        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)\n","        self.gamma = gamma  # Discount parameter $\\phi$\n","\n","    def learn(self, log_probs, advantages):\n","        loss = 0   # Surrogate advantage\n","        \n","        # Compute search direction (Using conjugate gradient algorithm)\n","        \n","\n","        # Update policy network by backpropagating line search\n","\n","        # (Option) Fit value function\n","\n","    def sample(self, state):\n","        # Compute the probability of each action with the observation from the environment\n","        action_prob = self.network(torch.FloatTensor(state))\n","        action_dist = Categorical(action_prob)\n","\n","        # Sample an action\n","        action = action_dist.sample()\n","\n","        # The probability of the sampled action\n","        log_prob = action_dist.log_prob(action)\n","\n","        return action.item(), log_prob"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ykOtl8DtUA47","colab_type":"text"},"source":["### Estimation: State-Action Value Function\n","\n","$$\\mathcal{Q}_{\\theta_{old}}(s_t, a_t) = \\sum_{l=0}^T \\gamma^lr(s_{t+l})$$\n"]},{"cell_type":"code","metadata":{"id":"dppGOaVkT48h","colab_type":"code","colab":{}},"source":["def computeQ(episode_rewards, gamma):\n","    Qvalues = episode_rewards.copy()\n","    gamma = gamma\n","    for i in reversed(range(len(Qvalues))):\n","            if (i+1) < len(Qvalues):\n","                Qvalues[i] = Qvalues[i] + gamma*Qvalues[i+1]\n","    return Qvalues"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xySWAoqENpIF","colab_type":"text"},"source":["### Train"]},{"cell_type":"markdown","metadata":{"id":"sLqXHVvaL7EE","colab_type":"text"},"source":["#### Single Path"]},{"cell_type":"code","metadata":{"id":"8f_AqHZJNr0S","colab_type":"code","colab":{}},"source":["# Initialize policy network $\\theta_0$\n","network = PolicyGradientNetwork()\n","\n","# Initialize policy agent $\\gamma$\n","agent = TRPOAgent(network=network)\n","\n","# Hyperparameters\n","delta = 0       # KL-divergence limit $\\delta$\n","alpha = 0       # Backpropagation coefficient $\\alpha$\n","NUM_BATCH = 0   # Maximum number of backpropagation steps $K$"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b123xY4YRau5","colab_type":"code","colab":{}},"source":["# Turn the neural network to training mode\n","agent.network.train()\n","\n","# Hyperparameters\n","EPISODE_PER_BATCH = 5  # Update agent once per EPISODE_PER_BATCH episodes.\n","\n","# Log parameters\n","avg_total_rewards, avg_final_rewards = [], []\n","\n","# Start training\n","prg_bar = tqdm_notebook(range(NUM_BATCH))\n","for batch in prg_bar:\n","\n","    log_probs = []\n","    total_rewards, final_rewards, batch_Qvalues = [], [], [], []\n","\n","    # Collect training data\n","    for episode in range(EPISODE_PER_BATCH):\n","        \n","        state = env.reset()\n","        total_reward, total_step = 0, 0\n","\n","        episode_rewards, episode_Qvalues = [], []\n","\n","        while True:\n","\n","            action, log_prob = agent.sample(state)\n","            next_state, reward, done, _ = env.step(action)\n","\n","            log_probs.append(log_prob)\n","            state = next_state\n","            total_reward += reward\n","            total_step += 1\n","\n","            episode_rewards.append(reward)\n","\n","            if done:\n","                final_rewards.append(reward)\n","                total_rewards.append(total_reward)\n","                episode_Qvalues = computeQ(episode_rewards, agent.gamma)\n","\n","                batch_Qvalues.append(episode_Qvalues)\n","                break\n","\n","    # Log training process\n","    avg_total_reward = sum(total_rewards) / len(total_rewards)\n","    avg_final_reward = sum(final_rewards) / len(final_rewards)\n","    avg_total_rewards.append(avg_total_reward)\n","    avg_final_rewards.append(avg_final_reward)\n","    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n","\n","    # Q values normalization and standardization\n","    batch_Qvalues = np.concatenate(batch_Qvalues, axis=0)\n","    batch_Qvalues = (batch_Qvalues - np.mean(batch_Qvalues)) / np.std(batch_Qvalues) + 1e-9\n","    # Update Policy Gradient Network\n","    agent.learn(torch.stack(log_probs), torch.from_numpy(batch_Qvalues))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wR5ejxuUL_H6","colab_type":"text"},"source":["#### Vine"]},{"cell_type":"code","metadata":{"id":"4hzsVRvBL_rV","colab_type":"code","colab":{}},"source":["network = PolicyGradientNetwork()\n","agent = TRPOAgent(network)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5QLp1JHeMGD","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PAHjEYrebIH","colab_type":"text"},"source":["### Test"]},{"cell_type":"code","metadata":{"id":"OHVlIQkEeduq","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}