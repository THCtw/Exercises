{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TRPO_LunarLander.ipynb","provenance":[],"collapsed_sections":["Mtvl_qfdhEGD"],"toc_visible":true,"authorship_tag":"ABX9TyMAtU4CHrH3dyYo6SSZLdgD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nhpEWJtKf6xG","colab_type":"text"},"source":["# **Trust Region Policy Optimization**\n","\n","Environment: Lunar Lander"]},{"cell_type":"markdown","metadata":{"id":"RwQi6GqRgcB3","colab_type":"text"},"source":["## References\n","\n","#### Papers\n","- [Trust Region Policy Optimization, Schulman et al. 2015](https://arxiv.org/abs/1502.05477)\n","- [High-Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al, 2015. Algorithm: GAE.](https://arxiv.org/abs/1506.02438)\n","- [Approximately Optimal Approximate Reinforcement Learning, Kakade and Langford 2002](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf)\n","\n","#### Blogs\n","- [OpenAI Spinning Up - Trust Region Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/trpo.html)\n","\n","#### Others\n","- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/index.html)\n","- [OpenAI Gym](https://gym.openai.com/)"]},{"cell_type":"markdown","metadata":{"id":"Mtvl_qfdhEGD","colab_type":"text"},"source":["## Preparation"]},{"cell_type":"code","metadata":{"id":"7JPQqFATf9ix","colab_type":"code","colab":{}},"source":["%%capture\n","!sudo apt update\n","!sudo apt install python-opengl xvfb -y\n","!pip install gym[box2d] pyvirtualdisplay piglet tqdm\n","\n","%%capture\n","from pyvirtualdisplay import Display\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","from IPython import display\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","from tqdm import tqdm_notebook"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SkF5P95Yg9CV","colab_type":"code","colab":{}},"source":["# import gym and create a Lunar Lander environment\n","%%capture\n","import gym\n","env = gym.make('LunarLander-v2')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czBoaD93lUQ6","colab_type":"text"},"source":["## TRPO Algorithm\n","Trust Region Policy Optimization algorithm from the [original paper](https://arxiv.org/abs/1502.05477)\n","\n","> Initialize $\\pi_0$.\n","> <br>**for** i = 0, 1, 2,... until convergence **do**\n","\n","> > Compute all advantage values $A_{\\pi_i}(s, a)$. \n","> > <br> Solve the constrained optimization problem:\n","> > <br>\n","> > <br> $\\pi_{i+1} = \\underset{\\pi}{arg\\ max}\\ L_{\\pi_i}(\\pi)$ $~~~~$ s.t. $\\bar{D}^{\\rho_{\\pi_i}}_{KL}(\\pi_i, \\pi) \\leq \\delta$\n","> > <br> where $L_{\\pi_i}(\\pi) = \\eta(\\pi_i) + \\underset{s}{\\sum} \\rho_{\\pi_i}(s) \\underset{a}{\\sum} \\pi(a|s) A_{\\pi_i}(s, a)$\n","\n","> > which is equivalent to solve:\n","> > <br> $\\underset{\\theta}{maximize}\\ \\underset{s}{\\sum} \\rho_{\\theta_{old}}(s) \\underset{a}{\\sum} \\pi_\\theta(a|s) A_{\\theta_{old}}(s, a)$ $~~~~$ s.t. $\\bar{D}^{\\rho_{\\theta_{old}}}_{KL}(\\theta_{old}, \\theta) \\leq \\delta$\n","\n","> **end for**\n","\n","<br> Next, we are trying to approximate the objective and constraint functions using Monte Carlo simulation.\n","\n","1. Replace $\\underset{s}{\\sum} \\rho_{\\theta_{old}}(s)[...]$ by $\\frac{1}{1-\\gamma}\\mathbb{E}_{s \\sim \\rho_{\\theta_{old}}}[...]$.\n","2. Replace advantage values function $A_{\\theta_{old}}$ by the state-action value function $Q_{\\theta_{old}}$.\n","3. Replace sum over actions by importance sampling estimator ($q$ is the sampling distribution):\n","> $\\underset{a}{\\sum} \\pi_\\theta(a|s) A_{\\theta_{old}}(s_n, a) = \\mathbb{E}_{a \\sim q}\\ [\\frac{\\pi_{\\theta}(a|s_n)}{q(a|s_n)} A_{\\theta_{old}}(s_n, a)]$\n","\n","<br> The optimization problem become:\n","\n","> $\\underset{\\theta}{maximize}\\ \\mathbb{E}_{s \\sim \\rho_{\\theta_{old}},\\ a \\sim q}\\ [\\frac{\\pi_{\\theta}(a|s)}{q(a|s)} Q_{\\theta_{old}}(s, a)]$ $~~~~$ s.t. $\\mathbb{E}_{s \\sim \\rho_{\\theta_{old}}}\\ [\\ D_{KL}(\\ \\pi_{\\theta_{old}}(\\cdot|s)\\ \\|\\ \\pi_{\\theta}(\\cdot|s)\\ )\\ ] \\leq \\delta$\n","\n","<br> The remaining steps are:\n","\n","1. Replace the expectations by sample averages.\n","2. Replace the $Q$ value by an empirical estimate.\n","\n","In the original paper, the author provided two schemes, single path and vine, for the these steps. We will implement both of them."]},{"cell_type":"markdown","metadata":{"id":"6mL7MQAJ0Q13","colab_type":"text"},"source":["### Single Path\n","\n","In this scheme, we generate a trajectory ($s_0, a_0, ... , s_{T-1}, a_{T-1}, s_T$) by $\\rho_0$ and $\\pi_{old}$, which means $p(a|s) = \\pi_{old}(a|s)$, and compute the $Q$ value as $$\\hat{\\mathcal{Q}}_{\\theta_{old}}(s_t, a_t) = \\sum_{\\tau\\in\\theta_{old}} \\sum_{l=0}^T \\gamma^lr(s_{t+l})$$"]},{"cell_type":"markdown","metadata":{"id":"9F-S6BEEDZPY","colab_type":"text"},"source":["### Policy Gradient Network\n","\n","> Fisrt, we construct the same policy gradient neural network as [TRPO paper](https://arxiv.org/abs/1502.05477)."]},{"cell_type":"code","metadata":{"id":"Fv7wff7vlXPI","colab_type":"code","colab":{}},"source":["class PolicyGradientNetwork(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(8, 16)\n","        self.fc2 = nn.Linear(16, 16)\n","        self.fc3 = nn.Linear(16, 4)\n","\n","    def forward(self, state):\n","        hid = torch.tanh(self.fc1(state))\n","        hid = torch.tanh(self.fc2(hid))\n","        return F.softmax(self.fc3(hid), dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-itJ6AREMEq","colab_type":"text"},"source":["### Trust Region Policy Optimization Agent\n","\n","> Next, we build an TRPO agent which take the policy gradient network abouve to take action and have the following functions:\n","1. `learn()`:"]},{"cell_type":"code","metadata":{"id":"hnLlIwRwD_1z","colab_type":"code","colab":{}},"source":["class TRPOAgent():\n","\n","    def __init__(self, network):\n","        self.network = network\n","        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ykOtl8DtUA47","colab_type":"text"},"source":["### Compute State-Action Value Function Estimates\n","\n","$$\\hat{\\mathcal{Q}}_{\\theta_{old}}(s_t, a_t) = \\sum_{\\tau\\in\\theta_{old}} \\sum_{l=0}^T \\gamma^lr(s_{t+l})$$\n"]},{"cell_type":"code","metadata":{"id":"dppGOaVkT48h","colab_type":"code","colab":{}},"source":["def computeQ():\n","\n","    return Qvalues"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xySWAoqENpIF","colab_type":"text"},"source":["### Train"]},{"cell_type":"markdown","metadata":{"id":"sLqXHVvaL7EE","colab_type":"text"},"source":["#### Single Path"]},{"cell_type":"code","metadata":{"id":"8f_AqHZJNr0S","colab_type":"code","colab":{}},"source":["network = PolicyGradientNetwork()\n","agent = TRPOAgent(network)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b123xY4YRau5","colab_type":"code","colab":{}},"source":["agent.network.train() \n","EPISODE_PER_BATCH = 5  # Update agent once per EPISODE_PER_BATCH episodes.\n","NUM_BATCH = 400        # Update agent NUM_BATCH times in total.\n","gamma = 0.5            # Discount parameter\n","\n","avg_total_rewards, avg_final_rewards = [], []\n","\n","prg_bar = tqdm_notebook(range(NUM_BATCH))\n","for batch in prg_bar:\n","\n","    log_probs = []\n","    total_rewards, final_rewards = [], []\n","\n","    discounted_rewards = []\n","\n","    # Collect training data\n","    for episode in range(EPISODE_PER_BATCH):\n","        \n","        state = env.reset()\n","        total_reward, total_step = 0, 0\n","\n","        episode_reward = []\n","        \n","\n","        while True:\n","\n","            action, log_prob = agent.sample(state)\n","            next_state, reward, done, _ = env.step(action)\n","\n","            log_probs.append(log_prob)\n","            state = next_state\n","            total_reward += reward\n","            total_step += 1\n","\n","            episode_reward.append(reward)\n","\n","            if done:\n","                final_rewards.append(reward)\n","                total_rewards.append(total_reward)\n","                                \n","                discounted_reward = discount(episode_reward, gamma)\n","                discounted_rewards.append(discounted_reward)\n","                break\n","\n","    # Log training process\n","    avg_total_reward = sum(total_rewards) / len(total_rewards)\n","    avg_final_reward = sum(final_rewards) / len(final_rewards)\n","    avg_total_rewards.append(avg_total_reward)\n","    avg_final_rewards.append(avg_final_reward)\n","    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n","\n","    # Update Policy Gradient Network\n","    discounted_rewards = np.concatenate(discounted_rewards, axis=0)\n","    discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / np.std(discounted_rewards) + 1e-9\n","    agent.learn(torch.stack(log_probs), torch.from_numpy(discounted_rewards), EPISODE_PER_BATCH)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wR5ejxuUL_H6","colab_type":"text"},"source":["#### Vine"]},{"cell_type":"code","metadata":{"id":"4hzsVRvBL_rV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}