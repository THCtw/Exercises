{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PPO_LunarLander.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNzEUhXql+DpshvCoo5zbOU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WGtE_YwaB5-E","colab_type":"text"},"source":["# **Proximal Policy Optimization**\n","\n","Environment: Lunar Lander"]},{"cell_type":"markdown","metadata":{"id":"wPLqDX2ypB0K","colab_type":"text"},"source":["## References\n","\n","#### Papers\n","- [Proximal Policy Optimization Algorithms, Schulman et al. 2017](https://arxiv.org/abs/1707.06347)\n","- [Emergence of Locomotion Behaviours in Rich Environments, Heess et al. 2017](https://arxiv.org/abs/1707.02286)\n","\n","#### Blogs\n","- [OpenAI Spinning Up - Proximal Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/ppo.html)\n","\n","#### Others\n","- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/index.html)\n","- [OpenAI Gym](https://gym.openai.com/)"]},{"cell_type":"markdown","metadata":{"id":"hCTdFexBqAjO","colab_type":"text"},"source":["## Preparation"]},{"cell_type":"code","metadata":{"id":"oKgu1rhNqEHn","colab_type":"code","colab":{}},"source":["%%capture\n","!sudo apt update\n","!sudo apt install python-opengl xvfb -y\n","!pip install gym[box2d] pyvirtualdisplay piglet tqdm\n","\n","%%capture\n","from pyvirtualdisplay import Display\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","from IPython import display\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","from tqdm import tqdm_notebook"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmEcTtE8qG2t","colab_type":"code","colab":{}},"source":["# Import gym and create a Lunar Lander environment\n","# Observation/State: Box(8,)\n","# Action: Discrete(4)\n","%%capture\n","import gym\n","env = gym.make('LunarLander-v2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bigUrXAHqH8T","colab_type":"text"},"source":["## PPO Algorithm\n","Proximal Policy Optimization algorithm from the [original paper](https://arxiv.org/abs/1707.06347)"]},{"cell_type":"markdown","metadata":{"id":"Bio4tjf0qWSz","colab_type":"text"},"source":["### Pseudocode"]},{"cell_type":"markdown","metadata":{"id":"cxzQUix5qZ5M","colab_type":"text"},"source":["###  Policy Gradient Network\n","\n","> In this notebook, we implement two policy gradient networks, the one used as an example in hw15 of HY-Lee's course (PGNetLee) and the one in the [original paper](https://arxiv.org/abs/1707.06347) (PGNet)."]},{"cell_type":"code","metadata":{"id":"JjbNXHGkqTag","colab_type":"code","colab":{}},"source":["class PGNetLee(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(8, 16)\n","        self.fc2 = nn.Linear(16, 16)\n","        self.fc3 = nn.Linear(16, 4)\n","\n","    def forward(self, state):\n","        hid = torch.tanh(self.fc1(state))\n","        hid = torch.tanh(self.fc2(hid))\n","        return F.softmax(self.fc3(hid), dim=-1)\n","\n","\n","#class PGNet(nn.Module):\n","\n","#    def __init__(self):\n","#       super().__init__()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d5DR6UY4qnwM","colab_type":"text"},"source":["### Proximal Policy Optimization Agent\n","\n","In original paper, the authors compare several different surrogate objectives, including clipping $L^{CLIP}$,$\\ $ conservitive policy iteration (no clippling or penalty) $L^{CPI}$ and KL penalty (fixed and adaptive) $L^{PENfix}$ $L^{PENadp}$.$\\ $ Here we implement these different objectives in different agents."]},{"cell_type":"markdown","metadata":{"id":"m2jiW-x4wGeA","colab_type":"text"},"source":["#### Conservitive Policy Iteration"]},{"cell_type":"code","metadata":{"id":"F0N-qCb2yd43","colab_type":"code","colab":{}},"source":["class PPOAgentCPI():\n","\n","    def __init__(self, network):\n","        self.network = network"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acFWboRVwLiP","colab_type":"text"},"source":["#### Clipping"]},{"cell_type":"code","metadata":{"id":"wvaqjO7wwbsF","colab_type":"code","colab":{}},"source":["class PPOAgentCLIP():\n","\n","    def __init__(self, network):\n","        self.network = network"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f2hPL2XkwPsW","colab_type":"text"},"source":["#### KL Penalty (Fixed)\n"]},{"cell_type":"code","metadata":{"id":"p5cW6mL3ygac","colab_type":"code","colab":{}},"source":["class PPOAgentKLFix():\n","\n","    def __init__(self, network):\n","        self.network = network"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nc5GyxeLwVZd","colab_type":"text"},"source":["#### KL Penalty (Adpative)"]},{"cell_type":"code","metadata":{"id":"xcmJHEmIyjRk","colab_type":"code","colab":{}},"source":["class PPOAgentKLAdp():\n","\n","    def __init__(self, network):\n","        self.network = network"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"prqx0dF0zp0-","colab_type":"text"},"source":["### Estimator: Generalized Advantage Estimation"]},{"cell_type":"code","metadata":{"id":"TCm2P7rDzzdi","colab_type":"code","colab":{}},"source":["def computeGAE(episode_rewards, gamma):\n","\n","    return GAE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1lXjiJamqxej","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"CopqOkC80Fn0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lycqaM-jqzBU","colab_type":"text"},"source":["### Test"]},{"cell_type":"code","metadata":{"id":"SXIMCdDdqz4H","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}