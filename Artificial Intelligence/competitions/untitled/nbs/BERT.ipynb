{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[{"file_id":"1otHeXQmZzHs2wY3QNmjnm0lsh4sJCoti","timestamp":1593524240619}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMLfacCWLqVb0zLoosfcI7i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Lkyse9QsP4BL","colab_type":"text"},"source":["# **Bidirectional Encoder Representations from Transformers**\n","\n","This notebook implements Bidirectional Encoder Representations from Transformers(BERT), one of famous Transformer architectures, to create a text classification model as a simple baseline for T-Brain competition."]},{"cell_type":"markdown","metadata":{"id":"QfSLLkNKQeLC","colab_type":"text"},"source":["## BERT"]},{"cell_type":"code","metadata":{"id":"8UPC-4y-WSzS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976578689,"user_tz":-480,"elapsed":1122,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["%%capture\n","#!pip install transformers"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wa4qft_GTVVd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976581311,"user_tz":-480,"elapsed":3737,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["from transformers import BertTokenizer, BertForSequenceClassification\n","\n","PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n","NUM_LABELS = 2\n","tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31Gix-uITmYy","colab_type":"text"},"source":["## T-Brain Simple Baseline - BERT"]},{"cell_type":"markdown","metadata":{"id":"mMSVAkYuTtWn","colab_type":"text"},"source":["### Preparing"]},{"cell_type":"markdown","metadata":{"id":"Wnbd_lOhTy7_","colab_type":"text"},"source":["> First, please download the [dataset](https://gitlab.com/kvnkuol/bestline/-/archive/jc/bestline-jc.zip?path=JC/data) as a .zip file from GitLab prepared by J.C. as well as the official data from T-Brain. The following code will extract the .zip file and assign the path of data."]},{"cell_type":"code","metadata":{"id":"JrqK6VWDTu9h","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976581312,"user_tz":-480,"elapsed":3733,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["%%capture\n","#!unzip -o bestline-jc-JC-data.zip\n","\n","RAW_DATA_PATH = 'JC_tbrain_train_final_0701.csv'    # T-Brain\n","DATA_PATH = 'bestline-jc-JC-data/JC/data'           # J.C."],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJfzgHt1U0Wv","colab_type":"text"},"source":["### Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"kwtgNMdrYEvU","colab_type":"text"},"source":["> Loading data with `codecs` and `BeautifulSoup`."]},{"cell_type":"code","metadata":{"id":"wL1IaN73YPZN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976581313,"user_tz":-480,"elapsed":3729,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["import os\n","import pandas as pd\n","import codecs\n","from bs4 import BeautifulSoup"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rctKvCHXrC1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976581314,"user_tz":-480,"elapsed":3724,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["def load_data(crawled_data_path, original_data_path):\n","    \n","    raw_df = pd.read_csv(original_data_path) # Data provided by T-Brain\n","\n","    news = []   # News crawled by J.C.\n","    labels = [] # Labels, AML related or not.\n","\n","    for file in sorted(os.listdir(crawled_data_path)):\n","        # Get labels. Hint: Empty 'name' contains two characters '[]'.\n","        news_ID = int(file.split('_')[0])\n","        if len(raw_df.loc[news_ID-1, 'name']) > 2:\n","            labels.append(1)\n","        else:\n","            labels.append(0)\n","\n","        # Get news content.\n","        f = codecs.open(DATA_PATH + '/' + file, 'r', 'utf-8')\n","        content = BeautifulSoup(f.read()).get_text()\n","        news.append(content)\n","\n","    return news, labels"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"yi2d1GHfEtMN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976582901,"user_tz":-480,"elapsed":5305,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["news, labels = load_data(crawled_data_path=DATA_PATH, original_data_path=RAW_DATA_PATH)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQugfOWnoI-A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1594976582904,"user_tz":-480,"elapsed":5300,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}},"outputId":"fe08d859-848f-4c84-b59e-d40fb8073812"},"source":["print(news[0])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["量化交易追求絕對報酬有效對抗牛熊市近年來投資市場波動越來越明顯，追求低波動、絕對報酬的量化交易備受注目。專家表示，採用量化交易策略投資台股，不管是處於多頭或是空頭市場，績效及波動度均可領跑大盤，甚至比國內投資台股的股票型基金及ETF的波動率還低，表現也更為穩定。大數據時代來臨，風行歐美50年的量化交易儼然成為顯學，台灣亦開始重視此一趨勢發展，也因此，中華機率統計學會及台北科技大學管理學院攜手主辦，並由元大期貨、摩根亞太量化交易等公司擔任協辦單位，今(7/5)日舉辦「時間序列與量化交易研討會」，就目前熱門的量化交易、智能投資等相關議題進行研討。越來越多的基金公司重視量化交易，全球規模較大的避險基金多採行量化交易，包括橋水基金(BridgewaterAssociates)、AQR資產管理公司、曼氏集團(ManGroup)、文藝復興科技(RenaissanceTechnologies)等全球知名避險基金。摩根亞太集團董事長張堯勇指出，避險基金規模約為5兆美元，採取量化交易的基金規模約1兆美元，比重佔了20%，代表量化交易的操作績效好，才會有那麼高的比重。量化交易的操作績效不亞於價值投資及技術投資，被譽為數學天才、最賺錢的基金經理人，文藝復興對沖基金創始人詹姆斯·西蒙斯(JamesSimons)所管理的大獎章(Medallion)基金，便是典型的量化交易，績效表現優異，不僅勝過索羅斯的量子基金，也打敗了股神巴菲特的價值投資。近年來台灣也逐漸重視量化交易，摩根亞太量化交易公司今年開始將量化交易引進台股投資，是國內首家推出量化交易策略的公司，初期對象鎖定法人機構，進行私募投資。張堯勇表示，數學不只是一門學科，更是一項扭轉乾坤、轉敗為勝的競爭利器，量化交易就是將數學運用在股市投資，透過複雜、精密的推理計算，打敗股市。他指出，目前全球利率水平50位於年來新低，美國十年期公債殖利率只有2%，日本及一些歐洲國家甚至是負利率。歷史極低利率帶導致無風險及低風險工具的投資報酬率太低（例如：銀行存款、政府公債及投資等級公司債），無法對抗通貨膨脹及支付負債，所以資產配置必須增加較高風險的投資，如股票、高收益公司債或新興市場債券，但是這些投資工具波動很大，而且很容易產生虧損！他表示，長期來說，股市絕對是好的投資，報酬率也不差，但是如何選股是一門學問。目前全球60個主要股市交易所，總市值70兆美元，而台灣上市上櫃市值35兆台幣，面對全球及台灣最大的股市金礦，投資人應該善用「量化交易」這個超級挖礦機，以程式選股來掏金。張堯勇指出，量化投資具有低波動、絕對報酬、優於大盤的三大特點，摩根亞太量化交易模擬台股近8年來的績效，結果顯示，運用量化交易的年化報酬率及波動度均優於台股上市櫃指數，以2011年金融海嘯及2018年中美貿易戰為例，台股上市、上櫃指數均是負報酬，不過，摩根量化交易卻是正報酬，而且波動度也比較低。此外，由於台灣四大政府基金操作績效不彰，不少金融機構也面臨投資虧損問題，亦可透過量化交易提高投資報酬率。張堯勇指出，郵儲、勞退、勞保、及退撫等四大基金，總資產高達10兆台幣，但過去10年的投資報酬率不到3%，去年虧損120億，面臨提早破產的風險！另外台灣保險公司的總資產高達26兆台幣，過去賣出去的保單利率高達4-6%，但是去年的資產報酬率卻只不到0.4%，而且去年綜合損益虧損高達5,000億！金管會連下三道金牌：所有保險公司今年不準發放現金股利、部分保險公司必須儘快舉辦現金增資、以後賣的保單利率必須下降。另外台灣全體商業銀行也有高達43兆的總資產43兆，但是其資產報酬率僅只有0.7%。以上將近80兆台幣的資金，張堯勇指出，若將10%投資於股市，投資金額高達8兆，報酬率每增加1%，就有800億元的獲利，對於台灣的國家財政或金融機構的獲利，都有非常巨大的助益。\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"db33FqP3CTvi","colab_type":"text"},"source":["#### Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"-80dEAAxUdm0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976582905,"user_tz":-480,"elapsed":5292,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["from torch import nn\n","import torch\n","from torch.utils import data\n","from gensim.models import Word2Vec"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGoDtuUvKt6R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594976582905,"user_tz":-480,"elapsed":5284,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}},"outputId":"956fae93-90d1-42df-d379-f4d9ed18a5d6"},"source":["# Creating funcions, '__init__', '__getitem__' and '__len__', for dataloader.\n","class TBrainDataset(data.Dataset):\n","    \"\"\"\n","    Expected data shape like:(data_num, data_len)\n","    Data can be a list of numpy array or a list of lists\n","    input data shape : (data_num, seq_len, feature_dim)\n","    \n","    __len__ will return the number of data\n","    \"\"\"\n","    def __init__(self, X, y, tokenizer):\n","        self.data = X\n","        self.label = y\n","        self.tokenizer = tokenizer\n","\n","    def __getitem__(self, idx):\n","        if self.label is None: \n","            article = self.data[idx]\n","            label_tensor = None\n","        else:\n","            article = self.data[idx]\n","            label_tensor = torch.tensor(self.label[idx])\n","\n","        # 建立句子的 BERT tokens\n","        word_pieces = [\"[CLS]\"]\n","        tokens_article = self.tokenizer.tokenize(article)\n","        word_pieces += tokens_article\n","        len_article = len(word_pieces)\n","        \n","        # 將整個 token 序列轉換成索引序列\n","        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n","        tokens_tensor = torch.tensor(ids)\n","\n","        return (tokens_tensor, label_tensor)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","\n","\"\"\"\n","# Data augmentation\n","def augment(x, y, n):\n","    # n = Number of copies.\n","    for i, label in enumerate(y):\n","        if label == 1:\n","            for j in range(n):\n","                x = torch.cat([x, x[i].unsqueeze(0)], dim=0)\n","                y = torch.cat([y, y[i].unsqueeze(0)], dim=0)\n","    return x, y\n","\"\"\""],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic":{"type":"string"},"text/plain":["'\\n# Data augmentation\\ndef augment(x, y, n):\\n    # n = Number of copies.\\n    for i, label in enumerate(y):\\n        if label == 1:\\n            for j in range(n):\\n                x = torch.cat([x, x[i].unsqueeze(0)], dim=0)\\n                y = torch.cat([y, y[i].unsqueeze(0)], dim=0)\\n    return x, y\\n'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"MQweZoYNvKHJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976582906,"user_tz":-480,"elapsed":5276,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","def create_mini_batch(samples):\n","    tokens_tensors = [s[0] for s in samples]\n","    \n","    # 測試集有 labels\n","    if samples[0][1] is not None:\n","        label_ids = torch.stack([s[1] for s in samples])\n","    else:\n","        label_ids = None\n","    \n","    # zero pad 到同一序列長度\n","    tokens_tensors = pad_sequence(tokens_tensors, \n","                                  batch_first=True)\n","    \n","    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n","    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n","    masks_tensors = torch.zeros(tokens_tensors.shape, \n","                                dtype=torch.long)\n","    masks_tensors = masks_tensors.masked_fill(\n","        tokens_tensors != 0, 1)\n","    \n","    return tokens_tensors, masks_tensors, label_ids"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TTG63tETgH0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976582906,"user_tz":-480,"elapsed":5270,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["%%capture\n","# Ref: HW4 in the course ML2020 by Hung-yi Lee in NTU.\n","\n","# Hyper-parameters for creating datatloader\n","batch_size = 1    #128\n","article_len = 500   # Length of article\n","\n","# Preprocessing\n","train_x = news\n","y = labels\n","\n","# Deviding data to training data and validation data\n","X_train, X_val, X_test, y_train, y_val, y_test = train_x[:3000], train_x[3000:4000], train_x[4000:], y[:3000], y[3000:4000], y[4000:]\n","#X_train, y_train = augment(X_train, y_train, 4)\n","\n","# Create dataset for dataloader.\n","train_dataset = TBrainDataset(X=X_train, y=y_train, tokenizer=tokenizer)\n","val_dataset = TBrainDataset(X=X_val, y=y_val, tokenizer=tokenizer)\n","test_dataset = TBrainDataset(X=X_test, y=y_test, tokenizer=tokenizer)\n","\n","# Transforming dataset to batch of tensors\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, collate_fn=create_mini_batch, shuffle = True, num_workers = 8)\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = batch_size, collate_fn=create_mini_batch, shuffle = False, num_workers = 8)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQ-e8gtwi2TU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1594976582907,"user_tz":-480,"elapsed":5263,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}},"outputId":"9acb65ce-0279-4ad3-ee9d-5c6ea09fcf32"},"source":["# Get information from dataset\n","train_aml=0\n","for i in train_dataset.label:\n","    if i == 1:\n","        train_aml+=1\n","\n","print(\"Number of training data: {}\".format(len(train_dataset.label)))\n","print(\"Number of AML related news: {}\".format(train_aml))\n","print(\"Percent of AML related news: {:.2f}%\".format((train_aml)/len(train_dataset.label)*100))\n","\n","val_aml=0\n","for i in val_dataset.label:\n","    if i == 1:\n","        val_aml+=1\n","\n","print(\"\\nNumber of validation data: {}\".format(len(val_dataset.label)))\n","print(\"Number of AML related news: {}\".format(val_aml))\n","print(\"Percent of AML related news: {:.2f}%\".format((val_aml)/len(val_dataset.label)*100))\n","\n","test_aml=0\n","for i in test_dataset.label:\n","    if i == 1:\n","        test_aml+=1\n","\n","print(\"\\nNumber of Testing data: {}\".format(len(test_dataset.label)))\n","print(\"Number of AML related news: {}\".format(test_aml))\n","print(\"Percent of AML related news: {:.2f}%\".format((test_aml)/len(test_dataset.label)*100))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Number of training data: 3000\n","Number of AML related news: 211\n","Percent of AML related news: 7.03%\n","\n","Number of validation data: 1000\n","Number of AML related news: 65\n","Percent of AML related news: 6.50%\n","\n","Number of Testing data: 647\n","Number of AML related news: 52\n","Percent of AML related news: 8.04%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"070FpGB5UQzI","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"1x0pch9wSsU1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976582907,"user_tz":-480,"elapsed":5256,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["# Ref: HW4 in the course ML2020 by Hung-yi Lee in NTU.\n","import os\n","import torch\n","import argparse\n","import numpy as np\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from gensim.models import word2vec\n","from sklearn.model_selection import train_test_split"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAu75calUZES","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594976583819,"user_tz":-480,"elapsed":6161,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["# Ref: HW4 in the course ML2020 by Hung-yi Lee in NTU.\n","\n","# train.py\n","# 這個 block 是用來訓練模型的\n","\n","def evaluation(outputs, labels):\n","    # outputs => probability (float)\n","    # labels => labels\n","    outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\n","    outputs[outputs<0.5] = 0 # 小於 0.5 為負面\n","\n","    # Confusion matrix\n","    tn, fp = 0, 0   # True Negtive, False Positive\n","    fn, tp = 0, 0   # False Negtive, True Positive\n","    for i in range(len(outputs)):\n","        if outputs[i]==1:\n","            if outputs[i].item() == labels[i].item():\n","                tp += 1\n","            else:\n","                fp += 1\n","        else:\n","            if outputs[i].item() == labels[i].item():\n","                tn += 1\n","            else:\n","                fn += 1\n","\n","    # Precision\n","    if tp+fp == 0:\n","        prec = 0\n","    else:\n","        prec = tp/(tp+fp)\n","\n","    # Recall\n","    if tp+fn == 0:\n","        rec = 0\n","    else:\n","        rec = tp/(tp+fn)\n","    \n","    # F1 Score\n","    if prec+rec == 0:\n","        f1 = 0\n","    else:\n","        f1 = 2 * (prec*rec/(prec+rec))\n","\n","    # Number of correct predictions\n","    correct = torch.sum(torch.eq(outputs, labels)).item()\n","\n","    return correct, f1\n","\n","def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n","\n","    total = sum(p.numel() for p in model.parameters())\n","    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n","\n","    model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數\n","    \n","    criterion = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss\n","    t_batch = len(train) \n","    v_batch = len(valid) \n","    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給 optimizer，並給予適當的 learning rate\n","\n","    total_loss, total_acc, best_f1 = 0, 0, 0\n","    train_loss, train_f1, val_loss, val_f1 = [], [], [], []\n","\n","    for epoch in range(n_epoch):\n","        total_loss, total_acc, total_f1 = 0, 0, 0\n","\n","        # Training\n","        for i, (tokens_tensors, masks_tensors, label_ids) in enumerate(train):\n","            tokens_tensors = tokens_tensors.to(device, dtype=torch.long)\n","            masks_tensors = masks_tensors.to(device, dtype=torch.long)\n","            label_ids = label_ids.to(device, dtype=torch.float)\n","\n","            #inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n","            #labels = labels.to(device, dtype=torch.float) # device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n","            optimizer.zero_grad() # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\n","            out = model(input_ids=tokens_tensors, labels=label_ids) # 將 input 餵給模型\n","            #outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n","            loss, outputs = out[:2]\n","\n","            #loss = criterion(outputs, labels) # 計算此時模型的 training loss\n","            loss.backward() # 算 loss 的 gradient\n","            optimizer.step() # 更新訓練模型的參數\n","\n","            correct, f1 = evaluation(outputs, labels) # 計算此時模型的 training accuracy\n","\n","            \n","            total_acc += (correct / batch_size)\n","            total_loss += loss.item()\n","            total_f1 += f1\n","\n","            #print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n","        \n","        train_loss.append(total_loss/t_batch)\n","        train_f1.append(total_f1/t_batch)\n","        print('\\nEpoch: {}'.format(epoch+1))\n","        print('Train | Loss:{:.5f} Acc: {:.3f} F1 Score: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100, total_f1/t_batch))\n","\n","        # Validation\n","        model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n","        with torch.no_grad():\n","            total_loss, total_acc, total_f1 = 0, 0, 0\n","            for i, (tokens_tensors, masks_tensors, label_ids) in enumerate(valid):\n","                #inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n","                #labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n","                tokens_tensors = tokens_tensors.to(device, dtype=torch.long)\n","                masks_tensors = masks_tensors.to(device, dtype=torch.long)\n","                label_ids = label_ids.to(device, dtype=torch.float)\n","                \n","                outputs = model(input_ids=tokens_tensors, attention_mask=masks_tensors, labels=label_ids) # 將 input 餵給模型\n","                #outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n","                loss = criterion(outputs, labels) # 計算此時模型的 validation loss\n","                correct, f1 = evaluation(outputs, labels) # 計算此時模型的 validation accuracy\n","\n","                total_acc += (correct / batch_size)\n","                total_loss += loss.item()\n","                total_f1 += f1\n","\n","            val_loss.append(total_loss/v_batch)\n","            val_f1.append(total_f1/v_batch)\n","            print(\"Valid | Loss:{:.5f} Acc: {:.3f} F1 Score: {:.3f}\".format(total_loss/v_batch, total_acc/v_batch*100, total_f1/v_batch))\n","\n","            # 如果 validation 的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n","            if total_f1 > best_f1:\n","                best_f1 = total_f1\n","                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n","                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n","                print('saving model with f1 {:.3f}'.format(total_f1/v_batch))\n","\n","        print('-----------------------------------------------')\n","        model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）\n","\n","    return train_loss, train_f1, val_loss, val_f1"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBtV7qF--FNx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1594976592452,"user_tz":-480,"elapsed":8621,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}},"outputId":"847e579b-d497-4690-ff71-500548325935"},"source":["# Ref: HW4 in the course ML2020 by Hung-yi Lee in NTU.\n","\n","# Checking if gpu is availible.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Defining hyper-parameters\n","epoch = 50\n","lr = 0.001\n","\n","fix_embedding = True # Fixing embedding during training\n","model_dir = os.getcwd() # Model directory for checkpoint model\n","\n","# Creating model\n","NUM_LABELS\n","model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME) #, num_labels=3)\n","#model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n","model = model.to(device)\n","\n","# Starting training\n","#train_loss, train_f1, val_loss, val_f1 = training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"nJBfMysSA8VS","colab_type":"code","colab":{}},"source":["inputs = tokenizer(news[0], return_tensors=\"pt\")\n","labels = torch.tensor[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wj2OMzJrjP_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1594976593998,"user_tz":-480,"elapsed":8189,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}},"outputId":"7b4b0503-8a0c-4199-f4a4-ed67f4dcdcb7"},"source":["train_loss, train_f1, val_loss, val_f1 = training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["\n","start training, parameter total:102269186, trainable:102269186\n","\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-bcd2657462ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-55eff221f2aa>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(batch_size, n_epoch, lr, model_dir, train, valid, model, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m#labels = labels.to(device, dtype=torch.float) # device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 將 input 餵給模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;31m#outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         )\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    369\u001b[0m     ):\n\u001b[1;32m    370\u001b[0m         self_attention_outputs = self.attention(\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    313\u001b[0m     ):\n\u001b[1;32m    314\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         )\n\u001b[1;32m    317\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     ):\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"]}]},{"cell_type":"code","metadata":{"id":"X9nB_v_gOuSH","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1594967325107,"user_tz":-480,"elapsed":241365,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","plt.title(\"Training\")\n","plt.plot(train_loss, label='train_loss')\n","plt.plot(train_f1, label='train_f1')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVGmo0szPGv7","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1594967325109,"user_tz":-480,"elapsed":241360,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["plt.title(\"Validation\")\n","plt.plot(val_loss, label='val_loss')\n","plt.plot(val_f1, label='val_f1')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BhP1UiVUbmRH","colab_type":"text"},"source":["### Testing"]},{"cell_type":"code","metadata":{"id":"318sGM_oble0","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1594967325110,"user_tz":-480,"elapsed":241358,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = len(test_dataset), shuffle = False, num_workers = 8)\n","test_size = len(test_dataset)\n","test_model = torch.load(\"{}/ckpt.model\".format(model_dir))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RiwcbDzvbqsa","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1594967325110,"user_tz":-480,"elapsed":241353,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["print(test_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qB34WqxJbrTT","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1594967325111,"user_tz":-480,"elapsed":241351,"user":{"displayName":"張子軒","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg59EHpRXDSmMVSaXz7Ua5YYwFGtrKusXSDUrt3Og=s64","userId":"11622394988518650577"}}},"source":["# Testing\n","test_model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n","criterion = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss\n","with torch.no_grad():\n","    total_loss, total_acc, total_f1 = 0, 0, 0\n","    for i, (inputs, labels) in enumerate(test_loader):\n","        inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n","        labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n","        outputs = test_model(inputs) # 將 input 餵給模型\n","\n","        #outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n","        loss = criterion(outputs, labels) # 計算此時模型的 testing loss\n","        correct, f1 = evaluation(outputs, labels) # 計算此時模型的 testing accuracy\n","\n","        if correct==1:\n","            print(loss, labels)\n","\n","        total_acc += (correct / test_size)\n","        total_loss += loss.item()\n","        total_f1 += f1\n","\n","    val_loss.append(total_loss/test_size)\n","    val_f1.append(total_f1/test_size)\n","    print(\"Valid | Loss:{:.5f} Acc: {:.3f}% F1 Score: {:.3f}\".format(total_loss/test_size, total_acc/test_size*100, total_f1/test_size))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFKcHXRXUKM7","colab_type":"text"},"source":["## Acknowledge"]},{"cell_type":"markdown","metadata":{"id":"IlmHCb2hUkHf","colab_type":"text"},"source":["## References\n","\n","### Courses\n","* [Machine Learning (2020,Spring)](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html) by Hung-yi Lee in NTU.\n","\n","### Blogs\n","* [進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html) by LeeMeng"]}]}